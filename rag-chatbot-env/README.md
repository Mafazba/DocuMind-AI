# Dr.X RAG Chatbot

A FastAPI-based Retrieval-Augmented Generation (RAG) chatbot that processes PDF, DOCX, and XLSX files, answers queries in English and Arabic, and summarizes responses with ROUGE evaluation. It retrieves relevant document chunks using FAISS, generates answers with Ollama (`llama3.2`), translates with Hugging Face models, extracts entities with spaCy, and redacts PII with Presidio.

## Features
- Upload and process PDF, DOCX, XLSX files.
- Answer queries in English/Arabic with automatic language detection and translation.
- Retrieve document chunks using FAISS with Nomic embeddings.
- Summarize context or answers with BART, evaluated by ROUGE-1, ROUGE-2, ROUGE-L scores.
- Extract entities for English queries using spaCy.
- Redact PII (names, emails, etc.) using Presidio.
- Maintain session history (last 3 interactions, 30-minute timeout).

## Methodology
The chatbot follows a RAG pipeline:
1. **Ingestion**: Documents (PDF, DOCX, XLSX) are parsed using `pdfplumber`, `python-docx`, and `pandas`. Text is split into chunks (~500 characters) with `langchain-community`’s `CharacterTextSplitter` to balance context and efficiency.
2. **Embedding**: Chunks are embedded with `nomic-ai/nomic-embed-text-v1` ( Sentence Transformers), chosen for its balance of performance and size, and stored in a FAISS index for fast similarity search.
3. **Query Processing**:
   - Queries are embedded and matched against the FAISS index to retrieve top-k chunks.
   - Language detection (`langdetect`) identifies English or Arabic input.
   - Non-English queries are translated to English (`Helsinki-NLP/opus-mt-mul-en`) for processing, then back to the target language (`Helsinki-NLP/opus-mt-en-ar` for Arabic).
   - Answers are generated by `llama3.2` (Ollama), incorporating retrieved chunks and session history.
4. **Summarization**: Optional summarization of context or answers uses `facebook/bart-large-cnn`, with ROUGE scores to measure quality.
5. **NLP & Privacy**: spaCy extracts entities (nouns, proper nouns) from English queries; Presidio redacts PII in answers.
6. **Evaluation**: ROUGE-1, ROUGE-2, and ROUGE-L scores assess summarization quality, ensuring key information is preserved.

The system optimizes for low latency with lazy model loading, FAISS caching, and dynamic batch sizes. Session history (last 3 interactions) enhances context-aware responses.

## Significant Discoveries
- **Arabic Translation Challenges**: Initial issues with `Helsinki-NLP/opus-mt-en-ar` failing to load due to missing `sentencepiece` or corrupted cache were resolved by pinning `sentencepiece==0.2.0` and clearing `~/.cache/huggingface/hub`. A fallback to English ensures usability if translation fails.
- **ROUGE Evaluation**: ROUGE-L scores above 0.5 indicate effective summarization, preserving semantic structure. Stemming in `rouge-score` improved scores by matching synonyms (e.g., “immunity” vs. “immune”).
- **Embedding Efficiency**: `nomic-ai/nomic-embed-text-v1` outperformed larger models in FAISS retrieval speed while maintaining accuracy, critical for real-time queries.
- **PII Redaction**: Presidio’s anonymization caught sensitive data (e.g., names in medical PDFs) but required tuning to avoid over-redaction of medical terms.
- **Session History**: Limiting history to 3 interactions balanced context retention with memory usage, with a 30-minute timeout to clear stale sessions.

## Large Language Model (LLM)
- **Model Used**: `llama3.2` (Llama 3.2, lightweight variant).
- **Source**: Obtained via Ollama, an open-source platform for running LLMs locally.
- **How Obtained**:
  - Installed Ollama from [ollama.com](https://ollama.com/download).
  - Pulled `llama3.2` with:
    ```bash
    ollama pull llama3.2
    ```
  - Integrated via the `ollama` Python client (`ollama==0.3.3`).
- **Why Chosen**: `llama3.2` offers strong performance for text generation with low resource usage, ideal for local deployment. Its compatibility with Ollama simplifies setup and ensures privacy by avoiding cloud APIs.

## Embedding Model
- **Model Used**: `nomic-ai/nomic-embed-text-v1` (via `sentence-transformers`).
- **Details**: A compact, high-performance embedding model optimized for text similarity tasks, producing 768-dimensional vectors.
- **Why Chosen**: Balances accuracy and speed for FAISS-based retrieval, outperforming heavier models like BERT in latency. Its open-source nature aligns with project goals.
- **Integration**: Loaded with `sentence-transformers`, embeddings are generated for document chunks and queries, stored in `data/faiss_index.bin` for efficient cosine similarity search.

## Requirements
- Python 3.10+
- Ollama with `llama3.2`
- Windows/Linux/macOS

## Setup
1. **Clone Repo** (if hosted):
   ```bash
   git clone <repo-url>
   cd Dr.X
   ```

2. **Virtual Environment**:
   ```bash
   python -m venv rag-chatbot-env
   .\rag-chatbot-env\Scripts\activate  # Windows
   ```

3. **Install Packages**:
   ```bash
   pip install -r requirements.txt
   python -m spacy download en_core_web_sm
   ```
   See `requirements.txt`:
   ```
   fastapi==0.115.0
   uvicorn==0.30.6
   pdfplumber==0.11.4
   python-docx==1.1.2
   pandas==2.2.2
   langchain-community==0.3.0
   sentence-transformers==3.1.1
   faiss-cpu==1.8.0
   presidio-analyzer==2.2.355
   presidio-anonymizer==2.2.355
   spacy==3.7.6
   transformers==4.44.2
   torch==2.4.1
   langdetect==1.0.9
   rouge-score==0.1.2
   ollama==0.3.3
   sentencepiece==0.2.0
   numpy==1.26.4
   python-multipart==0.0.9
   ```

4. **Set Up Ollama**:
   ```bash
   ollama pull llama3.2
   ollama serve
   ```

## Usage
1. **Run Server**:
   ```bash
   cd D:\Onedrive\OneDrive - IGO Solutions Ltd\Documents\AIGO Base\Code Base\Dr.X
   .\rag-chatbot-env\Scripts\activate
   uvicorn main:app --host 0.0.0.0 --port 8000
   ```

2. **Ingest File**:
   ```bash
   curl -X POST http://localhost:8000/ingest -F "file=@sample.pdf"
   ```

3. **Query**:
   - English:
     ```bash
     curl -X POST http://localhost:8000/query -H "Content-Type: application/json" -d "{\"query\": \"What are new cancer treatments?\", \"top_k\": 5, \"target_language\": \"en\"}"
     ```
   - Arabic:
     ```bash
     curl -X POST http://localhost:8000/query -H "Content-Type: application/json" -d "{\"query\": \"ما هي العلاجات الجديدة للسرطان؟\", \"top_k\": 5, \"target_language\": \"ar\"}"
     ```

## Structure
```
Dr.X/
├── data/
│   ├── faiss_index.bin
│   ├── faiss_metadata.pkl
├── rag-chatbot-env/
├── main.py
├── README.md
├── requirements.txt
```

## Troubleshooting
- **Arabic Translation Error**:
   ```bash
   pip install transformers sentencepiece
   rmdir /S /Q %USERPROFILE%\.cache\huggingface\hub
   ```
- **Ollama**:
   ```bash
   ollama list
   ollama pull llama3.2
   ```
- Check logs:
   ```bash
   uvicorn main:app --log-level debug
   ```

## License
MIT

---

### Notes
- **Methodology**: Describes the RAG pipeline, covering ingestion, embedding, query processing, summarization, NLP, and evaluation, based on your project’s functionality.
- **Significant Discoveries**: Highlights key findings (e.g., Arabic model fix, ROUGE insights) from our interactions, especially the translation errors (`Helsinki-NLP/opus-mt-en-ar`).
- **LLM**: Details `llama3.2` via Ollama, including how you obtained it (simple download and pull).
- **Embedding Model**: Specifies `nomic-ai/nomic-embed-text-v1`, its purpose, and why it was chosen.
- **Requirements**: Includes the exact `requirements.txt` from your request, pinned for reproducibility.
- **Git-Friendly**: Matches your intent to push to Git (e.g., via GitHub Desktop), keeping files like `requirements.txt` and `.gitignore` (excluding `rag-chatbot-env/`, `data/`) ready.
- **Arabic Fix**: The troubleshooting section reflects the solution to your `sentencepiece` and tokenizer errors.
